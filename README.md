```
I would like to introduce the API of "Particle_collider" to everyone. The first part is the API for data synthesizer. Using this API allows the controlled synthesis of text line images. It supports the configuration of six features: background, foreground, corpus, font, font grayscale value, and font size. Below is an example of data synthesis using the data synthesizer. First, import the data synthesizer from particle_collider. Then configure the parameters for each feature in the cfg dictionary. Each feature has an enable parameter. 
The first feature is the corpus. When the corpus field is enabled, it will use the corpus in the list of data. If not enabled, the data needs to be passed as a string, and all synthesized data will use this string as the corpus. This is mainly used in local explainers. 
The second feature is the font. The font feature can support 70 fonts. Users only need to enter the font name to use it for data synthesis. If users need to add other fonts, they just need to put the font file into the font search path. 
The third feature is the font grayscale value. The threshold for the font grayscale value can be set from 0 to 255. 
The fourth feature is the font size. It is recommended to configure the font size based on the width of the synthesized image and the length of the corpus string. If the font size is not enabled, it will adaptively adjust the font size based on the length of each corpus and the image width. 
The fifth is the background, and this feature only needs to pass the image address of the background. The background eraser can also be called to obtain the background image in advance. 
The last one is the foreground. The foreground has many supported features, including 32 data augmentation methods, most of which are noise that may occur in document printing or scanning. Configuring the foreground is also relatively easy; just fill in the configuration parameters of the foreground into fg_path.
The samples_number parameter is used to control the number of synthesized data. Another parameter, BOX, is the position coordinates [x1, y1, x2, y2] of the text line. It also determines the size of the synthesized text line. Here, x1 and y1 are set to 0, which is recommended when only synthesizing text line images. x2 represents the width of the text line, and y2 represents the image height.
In addition to the main functions mentioned above, we also provide auxiliary tools such as text background eraser and font analyzer. Currently, the open-source font recognition algorithm's performance is not satisfactory. The core issue is that noise in the image can interfere with font recognition. We have designed a new font recognition algorithm. Its basic principle is to use the extraction of the text skeleton in two images, and then compare the structural similarity of the two skeletons. Extracting the text skeleton can effectively filter out the interference caused by various noises in the image. The following is the calling method using the font analyzer; just input two images, and the font similarity can be calculated.
The second part is the global explainer. First, import the global explainer from "Particle_Collider". Then configure the dataset used by the explainer. The CFG parameter can be left blank, in which case the default dataset will be used. When evaluating the model, pass the model URL address to the explainer, and the global explainer evaluation will be automatically completed. All the datasets mentioned above are predefined datasets, each of which implements different evaluation functions. For example, the BLUR dataset contains 11 sub-datasets, each with linearly increasing blurriness, and all other features are completely identical.
 If the global explainer needs to change the evaluation data, just modify the dataset address in the config.py configuration file, ensure that the dataset format is consistent, and provide the basic model accuracy in advance. The reason for providing the accuracy of the basic model in advance is to save the time required for evaluating the basic model. We can extract the accuracy of the basic model, so there is no need to evaluate it every time.
The third part is the local explainer API. Currently, this part is still being improved, and using the local explainer based on UI will be more intuitive and simple. When calling the local explainer API, there are many parameters to configure. Subsequent simplification of configuration parameters and automatic analysis of features of interpreted data, such as font recognition and font size analysis, will be implemented. Below is an example of using the local explainer API.

```
